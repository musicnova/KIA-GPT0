{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kbqMeB_pSLD",
        "outputId": "ae674bc5-f926-4ed8-d12a-564161c4b536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# ТЕСТИРОВАНИЕ V30 RELEASE\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ijs5xXopVdr",
        "outputId": "6d1e567e-7a0b-4c18-88c4-22d4fe4daf01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FAISS_KIA_V30'...\n",
            "remote: Enumerating objects: 2314, done.\u001b[K\n",
            "remote: Counting objects: 100% (725/725), done.\u001b[K\n",
            "remote: Compressing objects: 100% (333/333), done.\u001b[K\n",
            "remote: Total 2314 (delta 494), reused 554 (delta 389), pack-reused 1589\u001b[K\n",
            "Receiving objects: 100% (2314/2314), 44.21 MiB | 22.69 MiB/s, done.\n",
            "Resolving deltas: 100% (1340/1340), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/drive/MyDrive/KIA_TEST/FAISS_KIA_V30\n",
        "!mkdir -p /content/drive/MyDrive/KIA_TEST/FAISS_KIA_V30\n",
        "!git clone -b faiss/V30 https://github.com/musicnova/KIA-GPT0.git FAISS_KIA_V30\n",
        "!cp -r /content/FAISS_KIA_V30/knowledge/faiss_router /content/drive/MyDrive/KIA_TEST/FAISS_KIA_V30/faiss_router\n",
        "!cp -r /content/FAISS_KIA_V30/knowledge/faiss /content/drive/MyDrive/KIA_TEST/FAISS_KIA_V30/faiss\n",
        "!rm -r /content/FAISS_KIA_V30\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rPgxTCh4p0o7"
      },
      "outputs": [],
      "source": [
        "!rm -rf faiss_routing\n",
        "!cp -r /content/drive/MyDrive/KIA_TEST/FAISS_KIA_V30/faiss_router faiss_routing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xuZJ2qn6U-gi"
      },
      "outputs": [],
      "source": [
        "!rm -rf faiss\n",
        "!cp -r /content/drive/MyDrive/KIA_TEST/FAISS_KIA_V30/faiss faiss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsFGkXMMwcX3"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nH2KyinoxhE",
        "outputId": "9e53a66e-7559-47bb-e0fa-8f656eeadf8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken==0.4.0\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain==0.0.231\n",
            "  Downloading langchain-0.0.231-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai==0.27.8\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu==1.7.4\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gspread in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (4.1.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting pydantic==1.10.8\n",
            "  Downloading pydantic-1.10.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.4.0) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.4.0) (2.31.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (3.8.6)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.231)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk<0.0.21,>=0.0.20 (from langchain==0.0.231)\n",
            "  Downloading langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (2.8.7)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (1.23.5)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.231)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==1.10.8) (4.5.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from gspread) (2.17.3)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (0.22.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (4.9)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.231)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.231)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2>=0.9.1->oauth2client) (3.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.231) (3.0.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (5.3.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.231) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.231)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: faiss-cpu, pydantic, mypy-extensions, marshmallow, typing-inspect, tiktoken, openapi-schema-pydantic, langchainplus-sdk, openai, dataclasses-json, langchain\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.5.14 faiss-cpu-1.7.4 langchain-0.0.231 langchainplus-sdk-0.0.20 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-0.27.8 openapi-schema-pydantic-1.2.4 pydantic-1.10.8 tiktoken-0.4.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip  install  tiktoken==0.4.0  langchain==0.0.231 openai==0.27.8 faiss-cpu==1.7.4 gspread oauth2client nltk pydantic==1.10.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wNTJkb02qexK"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.docstore.document import Document\n",
        "import requests\n",
        "#database\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "#from langchain.document_loaders import TextLoader\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "import pathlib\n",
        "import subprocess\n",
        "import tempfile\n",
        "import ipywidgets as widgets\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import re\n",
        "import getpass\n",
        "import os\n",
        "import openai\n",
        "import tiktoken\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BAWhNQ0K7x6k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import platform\n",
        "\n",
        "def clear_screen():\n",
        "    system_platform = platform.system()  # определить операционную систему\n",
        "    if system_platform == \"Windows\":\n",
        "        os.system('cls')\n",
        "    else:\n",
        "        os.system('clear')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Lh42jUHVy3fq"
      },
      "outputs": [],
      "source": [
        "MODEL_GPT_3_5_TURBO_16K = ['gpt-3.5-turbo-16k', 0.003, 0.004]\n",
        "MODEL_GPT_3_5_TURBO = ['gpt-3.5-turbo', 0.0015, 0.002]  # 4,097 tokens\n",
        "MODEL_GPT_3_5_TURBO_INSTRUCT = ['gpt-3.5-turbo-instruct', 0.0015, 0.002]  # 4,097 tokens\n",
        "MODEL_GPT_4 = ['gpt-4', 0.03, 0.06]  # 8,192 tokens\n",
        "SELECT_MODEL_GPT = MODEL_GPT_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jSdHAWANqs7C"
      },
      "outputs": [],
      "source": [
        "# openai_key = getpass.getpass(\"OpenAI API Key:\")\n",
        "# os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "# openai.api_key = openai_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eAafR4VwqtFS"
      },
      "outputs": [],
      "source": [
        "def insert_newlines(textstr: str, max_len: int = 170) -> str:\n",
        "    words = textstr.split()\n",
        "    lines = []\n",
        "    current_line = \"\"\n",
        "    for word in words:\n",
        "        if len(current_line + \" \" + word) > max_len:\n",
        "            lines.append(current_line)\n",
        "            current_line = \"\"\n",
        "        current_line += \" \" + word\n",
        "    lines.append(current_line)\n",
        "    return \"\\n\".join(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1YFsrIhi4Yct"
      },
      "outputs": [],
      "source": [
        "# функция для загрузки документа по ссылке из гугл драйв\n",
        "def load_document_text(url: str) -> str:\n",
        "    # Extract the document ID from the URL\n",
        "    match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)\n",
        "    if match_ is None:\n",
        "        raise ValueError('Invalid Google Docs URL')\n",
        "    doc_id = match_.group(1)\n",
        "\n",
        "    # Download the document as plain text\n",
        "    response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')\n",
        "    response.raise_for_status()\n",
        "    text = response.text\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l6EU0C7uUFp"
      },
      "source": [
        "## Собрать новую базу FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YnSehGphiewU"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "import re\n",
        "\n",
        "def load_file_knowledge(file_path: str) -> str:\n",
        "    # Чтение текстового файла\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    headers_to_split_on = [\n",
        "        (\"#\", \"router\"),\n",
        "        (\"##\", \"Header2\"),\n",
        "        (\"###\", \"Header3\"),\n",
        "        (\"####\", \"Header4\"),\n",
        "    ]\n",
        "\n",
        "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "    md_header_splits = markdown_splitter.split_text(text)\n",
        "\n",
        "    # Предполагается, что FAISS и OpenAIEmbeddings были импортированы или определены где-то выше\n",
        "    vectordateBase = FAISS.from_documents(md_header_splits, OpenAIEmbeddings())\n",
        "\n",
        "    return vectordateBase\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9CrXhbfVqtP_"
      },
      "outputs": [],
      "source": [
        "## models_knowledge_base = load_file_knowledge('models.md')\n",
        "## technology_knowledge_base = load_file_knowledge('technology.md')\n",
        "## parts_knowledge_base = load_file_knowledge('parts.md')\n",
        "## oils_knowledge_base = load_file_knowledge('oils.md')\n",
        "## tech_knowledge_base = load_file_knowledge('tech.md')\n",
        "## accessories_knowledge_base = load_file_knowledge('accessories.md')\n",
        "## warranty_knowledge_base = load_file_knowledge('warranty.md')\n",
        "## service_knowledge_base = load_file_knowledge('service.md')\n",
        "## sales_knowledge_base = load_file_knowledge('sales.md')\n",
        "## apps_knowledge_base = load_file_knowledge('apps.md')\n",
        "## promotions_knowledge_base = load_file_knowledge('promotions.md')\n",
        "## info_knowledge_base = load_file_knowledge('info.md')\n",
        "## howto_knowledge_base = load_file_knowledge('howto.md')\n",
        "## #none_knowledge_base = load_file_knowledge('none.md')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3fiDyYSqvuA"
      },
      "source": [
        "## Проверка чанков **БЕЗ** сбора нового файла FAISS **БЕЗ** обращения к OpenAI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_hi8kkfq6TV",
        "outputId": "5484bd91-66db-411e-9195-38d7a59d364c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken==0.4.0 in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: langchain==0.0.231 in /usr/local/lib/python3.10/dist-packages (0.0.231)\n",
            "Requirement already satisfied: openai==0.27.8 in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: faiss-cpu==1.7.4 in /usr/local/lib/python3.10/dist-packages (1.7.4)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (4.1.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: pydantic==1.10.8 in /usr/local/lib/python3.10/dist-packages (1.10.8)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.4.0) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.4.0) (2.31.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (3.8.6)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (0.5.14)\n",
            "Requirement already satisfied: langchainplus-sdk<0.0.21,>=0.0.20 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (0.0.20)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (2.8.7)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (1.23.5)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (1.2.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==1.10.8) (4.5.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from gspread) (2.17.3)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (0.22.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (4.9)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.231) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.231) (0.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2>=0.9.1->oauth2client) (3.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.231) (3.0.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (5.3.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.231) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.231) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip  install  tiktoken==0.4.0  langchain==0.0.231 openai==0.27.8 faiss-cpu==1.7.4 gspread oauth2client nltk pydantic==1.10.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "upyg9fjgq-sW"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.docstore.document import Document\n",
        "import requests\n",
        "#database\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "#from langchain.document_loaders import TextLoader\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "import pathlib\n",
        "import subprocess\n",
        "import tempfile\n",
        "import ipywidgets as widgets\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import re\n",
        "import getpass\n",
        "import os\n",
        "import openai\n",
        "import tiktoken\n",
        "\n",
        "class bcolors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKCYAN = '\\033[96m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "\n",
        "\n",
        "HISTORY = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kzkg5bierBx_"
      },
      "outputs": [],
      "source": [
        "MODEL_GPT_3_5_TURBO_16K = ['gpt-3.5-turbo-16k', 0.003, 0.004]\n",
        "MODEL_GPT_3_5_TURBO = ['gpt-3.5-turbo', 0.0015, 0.002]  # 4,097 tokens\n",
        "MODEL_GPT_4 = ['gpt-4', 0.03, 0.06]  # 8,192 tokens\n",
        "SELECT_MODEL_GPT = MODEL_GPT_3_5_TURBO_16K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qd6-_EUrDby",
        "outputId": "e8172b6c-0914-4f01-925f-518f09c4e498"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ],
      "source": [
        "openai_key = getpass.getpass(\"OpenAI API Key:\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "openai.api_key = openai_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "M1eAM1XfqxWg"
      },
      "outputs": [],
      "source": [
        "# Загружаем векторный файл базы знаний! В коллаб надо в папку faiss загрузить 2 файла\n",
        "retriever_base = FAISS.load_local(\"faiss\", OpenAIEmbeddings())\n",
        "models_knowledge_base = FAISS.load_local(\"faiss_routing/faiss_models\", OpenAIEmbeddings())\n",
        "technology_knowledge_base = FAISS.load_local(\"faiss_routing/faiss_technology\", OpenAIEmbeddings())\n",
        "parts_knowledge_base = FAISS.load_local(\"faiss_routing/faiss_parts\", OpenAIEmbeddings())\n",
        "oils_knowledge_base = FAISS.load_local(\"faiss_routing/faiss_oils\", OpenAIEmbeddings())\n",
        "tech_knowledge_base = FAISS.load_local(\"faiss_routing/faiss_tech\", OpenAIEmbeddings())\n",
        "accessories_knowledge_base = FAISS.load_local(\"faiss_routing/faiss_accessories\", OpenAIEmbeddings())\n",
        "warranty_knowledge_base = FAISS.load_local(\"faiss_routing/faiss_warranty\", OpenAIEmbeddings())\n",
        "service_knowledge_base = FAISS.load_local(\"faiss_routing/faiss_service\", OpenAIEmbeddings())\n",
        "sales_knowledge_base = FAISS.load_local(\"faiss_routing/faiss_sales\", OpenAIEmbeddings())\n",
        "apps_knowledge_base = FAISS.load_local(\"faiss_routing/faiss_apps\", OpenAIEmbeddings())\n",
        "promotions_knowledge_base = FAISS.load_local(\"faiss_routing/faiss_promotions\", OpenAIEmbeddings())\n",
        "info_knowledge_base = FAISS.load_local(\"faiss_routing/faiss_info\", OpenAIEmbeddings())\n",
        "howto_knowledge_base = FAISS.load_local(\"faiss_routing/faiss_howto\", OpenAIEmbeddings())\n",
        "#none_knowledge_base = FAISS.load_local(\"faiss_routing/faiss_models\", OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLhnEzASR7G8",
        "outputId": "d52f1d41-27a2-4805-98a0-925606653f3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models: Good for answering questions about models kia auto\n",
            "technology: Good for answering questions about Kia technology.\n",
            "parts: Good for answering questions about Kia parts.\n",
            "oils: Good for answering questions about Kia engine oils.\n",
            "tech: Good for answering questions about technical problems with Kia vehicles.\n",
            "accessories: Good for answering questions about accessories used in Kia vehicles.\n",
            "warranty: Good for answering questions about Kia vehicle warranties.\n",
            "service: Good for answering Kia vehicle service questions.\n",
            "sales: Good for answering Kia car sales questions.\n",
            "apps: Good for answering questions about using Kia software.\n",
            "promotion: Good for answering questions about Kia promotions.\n",
            "info: Good for answering questions about Kia documents.\n",
            "howto: Good for answering questions about Kia tools.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "models_template = \"\"\"You are a very smart assistant for Kia. You know everything about KIA car models\n",
        "You are excellent at answering questions about Kia car models concisely and clearly.\n",
        "When you don't know the answer to a question, you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "technology_template = \"\"\"You are a very smart assistant for KIA. You know everything about the technologies used in KIA cars.\n",
        "You are excellent at answering questions briefly and clearly about the technologies used in KIA cars.\n",
        "When you don't know the answer to a question, you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "parts_template = \"\"\"You are a very smart assistant for KIA. You know all the information on spare parts for KIA cars.\n",
        "You are excellent at answering questions about spare parts for KIA cars briefly and clearly.\n",
        "When you don't know the answer to a question, you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "oils_template = \"\"\"You are a very smart assistant for KIA. You know all the information on engine oils for KIA cars.\n",
        "You are excellent at answering questions about engine oils for KIA cars briefly and clearly.\n",
        "When you don't know the answer to a question, you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "tech_template = \"\"\"You are a very smart assistant for KIA. You know everything about the maintenance of KIA cars.\n",
        "You are excellent at answering questions about KIA vehicle maintenance briefly and clearly.\n",
        "When you don't know the answer to a question, you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "accessories_template = \"\"\"You are a very smart assistant for KIA. You know all the information about accessories for KIA cars.\n",
        "You are excellent at answering questions about accessories for KIA cars concisely and clearly.\n",
        "When you don't know the answer to a question, you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "warranty_template = \"\"\"You are a very smart assistant for KIA. You know all the information about the warranty for KIA cars.\n",
        "You are excellent at answering questions about KIA vehicle warranties concisely and clearly.\n",
        "When you don't know the answer to a question, you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "service_template = \"\"\"You are a very smart assistant for KIA. You know all the information about servicing KIA cars.\n",
        "You are excellent at answering questions about service for KIA vehicles briefly and clearly.\n",
        "When you don't know the answer to a question, you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "sales_template = \"\"\"You are a very smart assistant for KIA. You know all the information about KIA car sales.\n",
        "You are excellent at answering questions about KIA car sales briefly and clearly.\n",
        "When you don't know the answer to a question, you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "apps_template = \"\"\"You are a very smart assistant for KIA. You know all the information about KIA software.\n",
        "You are excellent at answering questions about KIA software briefly and clearly.\n",
        "When you don't know the answer to a question, you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "promotion_template = \"\"\"You are a very smart assistant for KIA. You know everything about promotions of KIA company.\n",
        "You are excellent at answering questions about promotions of KIA company concisely and clearly.\n",
        "When you don't know the answer to a question, you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "info_template = \"\"\"You are a very smart assistant for KIA. You know all the information about KIA software.\n",
        "You are excellent at answering questions about KIA documents briefly and clearly.\n",
        "When you don't know the answer to a question, you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "howto_template = \"\"\"You are a very smart assistant for KIA. You know all the information about KIA software.\n",
        "You are excellent at answering questions about KIA tools briefly and clearly.\n",
        "When you don't know the answer to a question, you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"models\",\n",
        "        \"description\": \"Good for answering questions about models kia auto\",\n",
        "        \"prompt_template\": models_template,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"technology\",\n",
        "        \"description\": \"Good for answering questions about Kia technology.\",\n",
        "        \"prompt_template\": technology_template,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"parts\",\n",
        "        \"description\": \"Good for answering questions about Kia parts.\",\n",
        "        \"prompt_template\": parts_template,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"oils\",\n",
        "        \"description\": \"Good for answering questions about Kia engine oils.\",\n",
        "        \"prompt_template\": oils_template,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"tech\",\n",
        "        \"description\": \"Good for answering questions about technical problems with Kia vehicles.\",\n",
        "        \"prompt_template\": tech_template,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"accessories\",\n",
        "        \"description\": \"Good for answering questions about accessories used in Kia vehicles.\",\n",
        "        \"prompt_template\": accessories_template,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"warranty\",\n",
        "        \"description\": \"Good for answering questions about Kia vehicle warranties.\",\n",
        "        \"prompt_template\": warranty_template,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"service\",\n",
        "        \"description\": \"Good for answering Kia vehicle service questions.\",\n",
        "        \"prompt_template\": service_template,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"sales\",\n",
        "        \"description\": \"Good for answering Kia car sales questions.\",\n",
        "        \"prompt_template\": sales_template,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"apps\",\n",
        "        \"description\": \"Good for answering questions about using Kia software.\",\n",
        "        \"prompt_template\": apps_template,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"promotion\",\n",
        "        \"description\": \"Good for answering questions about Kia promotions.\",\n",
        "        \"prompt_template\": promotion_template,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"info\",\n",
        "        \"description\": \"Good for answering questions about Kia documents.\",\n",
        "        \"prompt_template\": info_template,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"howto\",\n",
        "        \"description\": \"Good for answering questions about Kia tools.\",\n",
        "        \"prompt_template\": howto_template,\n",
        "    },\n",
        "]\n",
        "from langchain.chains.router import MultiRetrievalQAChain\n",
        "retriever = retriever_base.as_retriever()\n",
        "llm = OpenAI()\n",
        "\n",
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    # README https://python.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents\n",
        "    # README https://github.com/langchain-ai/langchain/issues/7644\n",
        "    # README https://github.com/langchain-ai/langchain/issues/6444\n",
        "    destination_chains[name] = chain # FIXME MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, default_chain=ConversationChain(llm=llm, output_key=\"text\")) # FIXME # chain\n",
        "default_chain = ConversationChain(llm=llm, output_key=\"text\")\n",
        "\n",
        "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
        "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)\n",
        "print(destinations_str)\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
        "\n",
        "chain = MultiPromptChain(\n",
        "    router_chain=router_chain,\n",
        "    destination_chains=destination_chains,\n",
        "    default_chain=default_chain,\n",
        "    verbose=True,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dsEKLyifvEto"
      },
      "outputs": [],
      "source": [
        "def _summarize_topic( dialog):\n",
        "        messages = [\n",
        "            {\"role\": \"system\",\n",
        "             \"content\": \"Ты - ассистент консультанта, основанный на AI. Ты умеешь профессионально суммаризировать присланные тебе диалоги консультанта и клиента. Твоя задача - суммаризировать диалог, который тебе пришел.\"},\n",
        "            {\"role\": \"user\",\n",
        "             \"content\": \"Суммаризируй следующий диалог консультанта и клиента: \" + \" \".join(dialog)}\n",
        "        ]\n",
        "\n",
        "        completion = openai.ChatCompletion.create(\n",
        "            model=SELECT_MODEL_GPT[0],\n",
        "            messages=messages,\n",
        "            temperature=0.1,  # Используем более низкую температуру для более определенной суммаризации\n",
        "            max_tokens=1000  # Ограничиваем количество токенов для суммаризации\n",
        "        )\n",
        "\n",
        "        return completion.choices[0].message.content\n",
        "\n",
        "def answer_index(topic, temp=0.1, top_similar_documents=3):\n",
        "\n",
        "        router_result = router_chain.__call__({\"input\": topic})\n",
        "        chosen_route = router_result['destination']\n",
        "        next_inputs = router_result['next_inputs']\n",
        "        print(f\"Выбранный маршрут: {chosen_route}\")\n",
        "        print(f\"Следующие входные данные: {next_inputs}\")\n",
        "\n",
        "        unit_to_multiplier = {\n",
        "            'models':  retriever_base, # models_knowledge_base,\n",
        "            'technology': retriever_base, # technology_knowledge_base,\n",
        "            'parts':  retriever_base, # parts_knowledge_base,\n",
        "            'oils': retriever_base, # oils_knowledge_base,\n",
        "            'tech': retriever_base, # tech_knowledge_base,\n",
        "            'accessories': retriever_base, # accessories_knowledge_base,\n",
        "            'warranty': retriever_base, # warranty_knowledge_base,\n",
        "            'service': retriever_base, # service_knowledge_base,\n",
        "            'sales': retriever_base, # sales_knowledge_base,\n",
        "            'apps': retriever_base, # apps_knowledge_base,\n",
        "            'promotions': retriever_base, # promotions_knowledge_base,\n",
        "            'info': retriever_base, # info_knowledge_base,\n",
        "            'howto': retriever_base, # howto_knowledge_base\n",
        "        }\n",
        "\n",
        "\n",
        "        summarize_text = ''\n",
        "        if len(HISTORY) > 0:\n",
        "            summarize_text = \"Вот краткий обзор предыдущего диалога: \" + _summarize_topic(\n",
        "                [q + ' ' + (a if a is not None else '') for q, a in HISTORY])\n",
        "            print(f'САММАРИ \\n=== {summarize_text} \\n')\n",
        "\n",
        "        # Добавляем явное разделение между историей диалога и текущим вопросом\n",
        "        input_text = summarize_text + \"\\n\\nТекущий вопрос: \" + topic\n",
        "\n",
        "        # FIXME from langchain.vectorstores import FAISS\n",
        "        # FIXME from langchain.embeddings import OpenAIEmbeddings\n",
        "        # FIXME from langchain.prompts.example_selector import (\n",
        "        # FIXME     MaxMarginalRelevanceExampleSelector,\n",
        "        # FIXME     SemanticSimilarityExampleSelector,\n",
        "        # FIXME )\n",
        "        # FIXME from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "        # FIXME example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
        "        # FIXME     # The list of examples available to select from.\n",
        "        # FIXME     examples,\n",
        "        # FIXME     # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
        "        # FIXME     OpenAIEmbeddings(),\n",
        "        # FIXME     # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
        "        # FIXME     FAISS,\n",
        "        # FIXME     # The number of examples to produce.\n",
        "        # FIXME     k=2,\n",
        "        # FIXME )\n",
        "        # FIXME mmr_prompt = FewShotPromptTemplate(\n",
        "        # FIXME     # We provide an ExampleSelector instead of examples.\n",
        "        # FIXME     example_selector=example_selector,\n",
        "        # FIXME     example_prompt=example_prompt,\n",
        "        # FIXME     prefix=\"Give the antonym of every input\",\n",
        "        # FIXME     suffix=\"Input: {adjective}\\nOutput:\",\n",
        "        # FIXME     input_variables=[\"adjective\"],\n",
        "        # FIXME )\n",
        "        # FIXME # Input is a feeling, so should select the happy/sad example as the first one\n",
        "        # FIXME print(mmr_prompt.format(adjective=\"worried\"))\n",
        "        # FIXME # Let's compare this to what we would just get if we went solely off of similarity,\n",
        "        # FIXME # by using SemanticSimilarityExampleSelector instead of MaxMarginalRelevanceExampleSelector.\n",
        "        # FIXME example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "        # FIXME # The list of examples available to select from.\n",
        "        # FIXME examples,\n",
        "        # FIXME # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
        "        # FIXME OpenAIEmbeddings(),\n",
        "        # FIXME # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
        "        # FIXME FAISS,\n",
        "        # FIXME # The number of examples to produce.\n",
        "        # FIXME k=2,\n",
        "        # FIXME )\n",
        "        # FIXME similar_prompt = FewShotPromptTemplate(\n",
        "        # FIXME # We provide an ExampleSelector instead of examples.\n",
        "        # FIXME example_selector=example_selector,\n",
        "        # FIXME example_prompt=example_prompt,\n",
        "        # FIXME prefix=\"Give the antonym of every input\",\n",
        "        # FIXME suffix=\"Input: {adjective}\\nOutput:\",\n",
        "        # FIXME input_variables=[\"adjective\"],\n",
        "        # FIXME )\n",
        "        # FIXME print(similar_prompt.format(adjective=\"worried\"))\n",
        "        # README https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/schema/vectorstore.py\n",
        "        # README https://python.langchain.com/docs/use_cases/question_answering/vector_db_qa#vectorstore-retriever-options\n",
        "        # VIEWME https://www.youtube.com/watch?v=n0uPzvGTFI0\n",
        "        # VIEWME https://www.youtube.com/watch?v=v9IjDAwGkBY\n",
        "        # VIEWME https://www.youtube.com/watch?v=biS8G8x8DdA\n",
        "        # README https://python.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents\n",
        "\n",
        "        # https://stackoverflow.com/questions/77222944/issue-with-retrievalqa-invalid-retriever-value-when-using-multiple-retrievals\n",
        "\n",
        "        from langchain.chains.router import MultiRetrievalQAChain\n",
        "        retriever_infos = [\n",
        "           {\n",
        "               \"name\": \"car\",\n",
        "               \"description\": \"Good for answering questions about car topics\",\n",
        "               \"retriever\": unit_to_multiplier.get(chosen_route, None)\n",
        "           },\n",
        "           {\n",
        "               \"name\": \"other\",\n",
        "               \"description\": \"Good for answering questions about other topics\",\n",
        "               \"retriever\": retriever\n",
        "           }\n",
        "        ]\n",
        "\n",
        "        knowledge_base = unit_to_multiplier.get(chosen_route, None) # MultiRetrievalQAChain.from_retrievers(llm, retriever_infos, default_chain=default_chain) # FIXME\n",
        "        if knowledge_base is not None:\n",
        "            docs = knowledge_base.max_marginal_relevance_search(topic, k=top_similar_documents, lambda_mult=0.25, fetch_k=50)\n",
        "        else:\n",
        "            return  insert_newlines('Вопрос не понятен!!!')\n",
        "\n",
        "\n",
        "        responses = []\n",
        "        for i, doc in enumerate(docs):\n",
        "            score = 0.5\n",
        "            if score < 1: # ТУТ ТЫ МОЖЕШЬ УПРАВЛЯТЬ праметром Л2 для чанков. 0..1\n",
        "                content = doc.page_content\n",
        "                response = f'\\n=======Отрывок документа №{i + 1}=====\\n{content}\\n'\n",
        "                print(f'\\n=====================Отрывок документа №{i + 1}=====================\\n')\n",
        "                print(f'=== score = {score}  Metadata документа ------------ {doc.metadata}')\n",
        "                print(f'\\n{content}\\n')\n",
        "                responses.append(response)\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": prompt},\n",
        "            {\"role\": \"user\",\n",
        "             \"content\": f\"Документ с информацией для ответа клиенту: {responses}\\n\\nВопрос клиента: \\n{input_text}\"}\n",
        "        ]\n",
        "\n",
        "        completion = openai.ChatCompletion.create(\n",
        "            model=SELECT_MODEL_GPT[0],\n",
        "            messages=messages,\n",
        "            temperature=temp\n",
        "        )\n",
        "\n",
        "        answer = completion.choices[0].message.content\n",
        "\n",
        "        # Добавляем вопрос пользователя и ответ системы в историю\n",
        "        HISTORY.append((topic, answer if answer is not None else ''))\n",
        "\n",
        "        return  insert_newlines(answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "g8N4WuC7qtKO"
      },
      "outputs": [],
      "source": [
        "HISTORY = []\n",
        "def run_dialog():\n",
        "    import time\n",
        "    HISTORY = []\n",
        "    AUTO_HELLO=\"Иван. Мне 10 лет. У папы сломалась KIA RIO в стандартной комплектации.\"\n",
        "    topic = 'Приветствую, я нейро-помощник KIA. Подскажите, какая у Вас марка машины? Как я могу к Вам обращаться?'\n",
        "    print('\\nМенеджер: ', topic+'\\n\\n')\n",
        "    print('\\nКлиент: ', AUTO_HELLO+'\\n\\n')\n",
        "    user_question = AUTO_HELLO\n",
        "    answer = answer_index(user_question)\n",
        "    HISTORY.append((topic, answer if answer is not None else ''))\n",
        "    print('\\nМенеджер: ', 'Какая Вам требуется помощь?'+'\\n\\n')\n",
        "    while True:\n",
        "        user_question = input('\\nКлиент: ')\n",
        "        if ((user_question.lower() == 'stop') or (user_question.lower() == 'стоп')):\n",
        "            break\n",
        "        try:\n",
        "            answer = answer_index(user_question)\n",
        "        except:\n",
        "            time.sleep(1)\n",
        "            HISTORY = []\n",
        "            # answer_index(None, None, None, knowledge_only_langchain, AUTO_HELLO)\n",
        "        print('\\nМенеджер: ', answer+'\\n\\n')\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YCBakZx6qtVM"
      },
      "outputs": [],
      "source": [
        "# Промпт моделей машин\n",
        "prompt = load_document_text ('https://docs.google.com/document/d/1i8HA7cX4Ut-tb9rf8wOgERU7lLe66xJYscizGtSSJl0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWY1AS2GvTN5",
        "outputId": "2244009b-58ca-479c-d816-31a4328c3f41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Менеджер:  Приветствую, я нейро-помощник KIA. Подскажите, какая у Вас марка машины? Как я могу к Вам обращаться?\n",
            "\n",
            "\n",
            "\n",
            "Клиент:  Иван. Мне 10 лет. У папы сломалась KIA RIO в стандартной комплектации.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Выбранный маршрут: tech\n",
            "Следующие входные данные: {'input': 'У моего папы сломалась KIA RIO в стандартной комплектации. Что делать?'}\n",
            "\n",
            "=====================Отрывок документа №1=====================\n",
            "\n",
            "=== score = 0.5  Metadata документа ------------ {'Header 1': 'sales_purchases', 'Header 2': 'sales_purchases154'}\n",
            "\n",
            "Автомобиль Киа Рио на механической коробке с объемом двигателя 1.6 черного цвета есть в наличии в городе Москва и Магнитогорск. Автомобили в Москве представлены в\n",
            "комплектации Comfort.\n",
            "\n",
            "\n",
            "=====================Отрывок документа №2=====================\n",
            "\n",
            "=== score = 0.5  Metadata документа ------------ {'Header 1': 'sevice_maintence', 'Header 2': 'sevice_maintence483'}\n",
            "\n",
            "Если ключ от автомобиля сломан, его замена должна производиться в официальных дилерских центрах, которые имеют необходимое оборудование и специально обученный персонал.\n",
            "Контакты дилеров можно найти на сайте: https://www.kia.ru/dealers/.\n",
            "\n",
            "\n",
            "=====================Отрывок документа №3=====================\n",
            "\n",
            "=== score = 0.5  Metadata документа ------------ {'Header 1': 'sevice_maintence', 'Header 2': 'sevice_maintence355'}\n",
            "\n",
            "Обработанная запись базы знаний: Гарантия на детали подвески и рулевого управления автомобиля Kia Carnival составляет 70 000 км пробега без ограничения по времени\n",
            "эксплуатации. Некоторые детали имеют ограниченный срок гарантии из-за естественного износа. Подробные условия гарантии можно найти в сервисной книжке и на сайте Kia.ru.\n",
            "Решение о проведении гарантийного ремонта принимает инженер по гарантии официального дилера Kia после осмотра/диагностики автомобиля.\n",
            "\n",
            "\n",
            "Менеджер:  Какая Вам требуется помощь?\n",
            "\n",
            "\n",
            "\n",
            "Клиент: Добрый день. Подскажите, пожалуйста, вчера проходил плановое техническое обслуживание по своему автомобилю Kia picanto JA 2017 г. в. И было выявлено то, что необходимо заменить правую форсунку омывателя лобового стекла. Я проходил техническое обслуживание у официального дилера в городе Екатеринбург, автобан на улице металлургов, 67. После работ была информация до меня донесена, что в наличии такой форсунки у них нет. Сегодня обзвонил всех дилеров имеющихся в городе Екатеринбурге по наличию именно в такой форсунки и ее нет ни у одного дилера. Подскажите, пожалуйста, могу ли я как-то через официальный сайт Kia сделать заявку заказ на поставку такой форсунки?\n",
            "\n",
            "Менеджер:   Я понимаю, что у вас возникли проблемы с KIA RIO в стандартной комплектации. Что именно произошло с автомобилем? Я постараюсь помочь вам с этим вопросом.\n",
            "\n",
            "\n",
            "\n",
            "Клиент: стоп\n"
          ]
        }
      ],
      "source": [
        "run_dialog()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}