{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ЧЕРНОВОЙ ВАРИАНТ парсит страницы веб сайта\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MKywTtpEefp",
        "outputId": "b9464fdc-20d1-43fb-9c19-0f896ca4f240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"config.txt\", \"w\") as fw:\n",
        "    fw.write(\"[COLAB]\\n\")\n",
        "    fw.write(\"knowledge_dir=/content/drive/MyDrive/KIA_AVATAR/Scripts/knowledge\\n\")\n",
        "    fw.write(\"prev_knowledge_dir=/content/drive/MyDrive/KIA_AVATAR/Scripts/../knowledge\\n\")\n",
        "    fw.write(\"\\n\")\n",
        "    fw.write(\"[PROM]\\n\")\n",
        "    fw.write(\"knowledge_dir=/opt/knowledge\\n\")\n",
        "    fw.write(\"prev_knowledge_dir=/knowledge\\n\")\n",
        "    fw.write(\"\\n\")\n",
        "    fw.write(\"[STEP03_COPY_WEBSITE]\\n\")\n",
        "    fw.write(\"api_key=?\\n\")\n",
        "    fw.write(\"neuro_copyrighter_limit=2\\n\")\n",
        "\n",
        "msg = \"\""
      ],
      "metadata": {
        "id": "2bSEj5eTEm-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "        import configparser\n",
        "        import pathlib\n",
        "        import shutil\n",
        "        import os\n",
        "        config = configparser.ConfigParser()\n",
        "        config.read('config.txt')\n",
        "        knowledge_dir = config[\"COLAB\"][\"knowledge_dir\"]\n",
        "        if knowledge_dir is None: knowledge_dir = \"./knowledge\"\n",
        "        pathlib.Path(knowledge_dir).mkdir(parents=True, exist_ok=True)\n",
        "        prev_knowledge_dir = config[\"COLAB\"][\"prev_knowledge_dir\"]\n",
        "        if prev_knowledge_dir is None: prev_knowledge_dir = \"../knowledge\"\n",
        "        pathlib.Path(prev_knowledge_dir).mkdir(parents=True, exist_ok=True)\n",
        "        shutil.copyfile(os.path.join(prev_knowledge_dir, \"parser_database.txt\"), os.path.join(knowledge_dir, \"parser_database.txt\"))\n",
        "        \"\"\"\n",
        "        import datetime\n",
        "        import numpy as np\n",
        "        import openpyxl\n",
        "        import re\n",
        "        import requests\n",
        "        import threading, time\n",
        "        import json\n",
        "        import os\n",
        "\n",
        "        from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "\n",
        "        def savetxt(dir, name, text):\n",
        "            if not os.path.exists('result'): os.mkdir('result')\n",
        "            if not os.path.exists('result/' + dir): os.mkdir('result/' + dir)\n",
        "            with open('result/' + dir + '/' + name, 'w', encoding='utf-8') as f:\n",
        "                f.write(text)\n",
        "            return True\n",
        "\n",
        "        def resulttotext(r):\n",
        "            if r is not None:\n",
        "                return r.text + '.'\n",
        "            else:\n",
        "                return ''\n",
        "\n",
        "        def parser(soup):\n",
        "            text = ''\n",
        "            pages = soup.findAll('div', class_='g-padding')\n",
        "            if pages is None:\n",
        "                print(soup.find('title').text.split(' – ')[0])\n",
        "                print(soup)\n",
        "            else:\n",
        "                for page in pages:\n",
        "                    if page.find('div', class_='faq') is not None:\n",
        "                        text += page.find('div', class_='faq').text\n",
        "                    else:\n",
        "                        content = page.select('div[class*=\"text-\"]')\n",
        "                        text += ''.join(i.text + '\\n' for i in content)\n",
        "            return text\n",
        "\n",
        "        # Pars page whith models\n",
        "\n",
        "        pagemodels = requests.get('https://www.kia.ru/models/').text\n",
        "        modellinks = []\n",
        "\n",
        "        html = ''.join(line.strip() for line in pagemodels.split(\"\\n\"))\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "        cards_list = soup.find_all('div', class_='car-card')\n",
        "\n",
        "        for card in cards_list:\n",
        "            modellinks.append('https://www.kia.ru' + str(card.a['href']))\n",
        "\n",
        "\n",
        "        def getmodelsoup(url):\n",
        "            model = requests.get(url).text\n",
        "            html = ''.join(line.strip() for line in model.split(\"\\n\"))\n",
        "            return BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "        modeldict = {}\n",
        "\n",
        "        for i in modellinks:\n",
        "            modeldict[i] = getmodelsoup(i)\n",
        "\n",
        "        def stringstolist(div):\n",
        "            '''Превращаем блок в список строк'''\n",
        "            lis = []\n",
        "            try:\n",
        "                for string in div.strings:\n",
        "                    if string not in [' ', '']:\n",
        "                        lis.append(string.text)\n",
        "            except:\n",
        "                lis = ['']\n",
        "            return lis\n",
        "\n",
        "        def parsermodel(soup):\n",
        "            text = ''\n",
        "\n",
        "            # Обрабатываю по id basic\n",
        "            basic = soup.select('div[id*=\"basic_\"]')\n",
        "            for div in basic:\n",
        "                # Перевожу строки в список, для удобного обращиния\n",
        "                textlist = stringstolist(div)\n",
        "                try:\n",
        "                    img = div.find('img')['data-src']\n",
        "                except:\n",
        "                    img = ''\n",
        "                text += f'## {textlist[0]}\\n{\". \".join(textlist[1:])}\\nФото {img}\\n\\n'\n",
        "\n",
        "            # Обрабатываю по id dizain\n",
        "            dizain = soup.select('div[id*=\"dizain_\"]')\n",
        "            for div in dizain:\n",
        "                if len(div['id'].split('_')) > 2: continue  # Пропускаем внутренние блоки\n",
        "                # Перевожу строки в список, для удобного обращиния\n",
        "                textlist = stringstolist(div)[:-1]\n",
        "                text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "                for t in textlist[3:]:\n",
        "                    text += t + '. '\n",
        "                text = text[:-2] + '\\n\\n'\n",
        "\n",
        "            # Обрабатываю по id eksterer\n",
        "            eksterer = soup.select('div[id*=\"eksterer_\"]')\n",
        "            for div in eksterer:\n",
        "                if len(div['id'].split('_')) > 2: continue  # Пропускаем внутренние блоки\n",
        "                # Перевожу строки в список, для удобного обращиния\n",
        "                textlist = stringstolist(div)[:-1]\n",
        "                text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "                for t in textlist[3:]:\n",
        "                    text += t + '. '\n",
        "                text = text[:-2] + '\\n\\n'\n",
        "\n",
        "            # Обрабатываю по id interer\n",
        "            interer = soup.select('div[id*=\"interer_\"]')\n",
        "            for div in interer:\n",
        "                if len(div['id'].split('_')) > 2: continue  # Пропускаем внутренние блоки\n",
        "                # Перевожу строки в список, для удобного обращиния\n",
        "                textlist = stringstolist(div)[:-1]\n",
        "                text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "                for t in textlist[3:]:\n",
        "                    text += t + '. '\n",
        "                text = text[:-2] + '\\n\\n'\n",
        "\n",
        "            # Обрабатываю по id style\n",
        "            style = soup.select('div[id*=\"style_\"]')\n",
        "            for div in style:\n",
        "                if len(div['id'].split('_')) > 2: continue  # Пропускаем внутренние блоки\n",
        "                # Перевожу строки в список, для удобного обращиния\n",
        "                textlist = stringstolist(div)[:-1]\n",
        "                text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "                for t in textlist[3:]:\n",
        "                    text += t + '. '\n",
        "                text = text[:-2] + '\\n\\n'\n",
        "\n",
        "            # Обрабатываю по id multimedia\n",
        "            multimedia = soup.select('div[id*=\"multimedia_\"]')\n",
        "            for div in multimedia:\n",
        "                if len(div['id'].split('_')) > 2: continue  # Пропускаем внутренние блоки\n",
        "                # Перевожу строки в список, для удобного обращиния\n",
        "                textlist = stringstolist(div)[:-1]\n",
        "                text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "                for t in textlist[3:]:\n",
        "                    text += t + '. '\n",
        "                text = text[:-2] + '\\n\\n'\n",
        "\n",
        "            # Обрабатываю по id tehnologii\n",
        "            tehnologii = soup.select('div[id*=\"tehnologii_\"]')\n",
        "            for div in tehnologii:\n",
        "                if len(div['id'].split('_')) > 2: continue  # Пропускаем внутренние блоки\n",
        "                # Перевожу строки в список, для удобного обращиния\n",
        "                textlist = stringstolist(div)[:-1]\n",
        "                text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "                for t in textlist[3:]:\n",
        "                    text += t + '. '\n",
        "                text = text[:-2] + '\\n\\n'\n",
        "\n",
        "            # Обрабатываю по id bezopasnost\n",
        "            bezopasnost = soup.select('div[id*=\"bezopasnost_\"]')\n",
        "            for div in bezopasnost:\n",
        "                if len(div['id'].split('_')) > 2: continue  # Пропускаем внутренние блоки\n",
        "                # Перевожу строки в список, для удобного обращиния\n",
        "                textlist = stringstolist(div)[:-1]\n",
        "                text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "                for t in textlist[3:]:\n",
        "                    text += t + '. '\n",
        "                text = text[:-2] + '\\n\\n'\n",
        "\n",
        "            # Обрабатываю по id teplye_opcii\n",
        "            teplye_opcii = soup.select('div[id*=\"teplye_opcii_\"]')\n",
        "            for div in teplye_opcii:\n",
        "                if len(div['id'].split('_')) > 3: continue  # Пропускаем внутренние блоки\n",
        "                # Перевожу строки в список, для удобного обращиния\n",
        "                textlist = stringstolist(div)[:-1]\n",
        "                text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "                for t in textlist[3:]:\n",
        "                    text += t + '. '\n",
        "                text = text[:-2] + '\\n\\n'\n",
        "\n",
        "            # Обрабатываю по id komfort\n",
        "            komfort = soup.select('div[id*=\"komfort_\"]')\n",
        "            for div in komfort:\n",
        "                if len(div['id'].split('_')) > 2: continue  # Пропускаем внутренние блоки\n",
        "                # Перевожу строки в список, для удобного обращиния\n",
        "                textlist = stringstolist(div)[:-1]\n",
        "                text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "                for t in textlist[3:]:\n",
        "                    text += t + '. '\n",
        "                text = text[:-2] + '\\n\\n'\n",
        "\n",
        "            # Обрабатываю по id vmestimost\n",
        "            vmestimost = soup.select('div[id*=\"vmestimost_\"]')\n",
        "            for div in vmestimost:\n",
        "                if len(div['id'].split('_')) > 2: continue  # Пропускаем внутренние блоки\n",
        "                # Перевожу строки в список, для удобного обращиния\n",
        "                textlist = stringstolist(div)[:-1]\n",
        "                text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "                for t in textlist[3:]:\n",
        "                    text += t + '. '\n",
        "                text = text[:-2] + '\\n\\n'\n",
        "\n",
        "            # Обрабатываю по id dvigatel\n",
        "            dvigatel = soup.select('div[id*=\"dvigatel_\"]')\n",
        "            for div in dvigatel:\n",
        "                if len(div['id'].split('_')) > 2: continue  # Пропускаем внутренние блоки\n",
        "                # Перевожу строки в список, для удобного обращиния\n",
        "                textlist = stringstolist(div)[:-1]\n",
        "                text += f'## {textlist[0]}\\n{\". \".join(textlist[1:3])}\\n'\n",
        "                text += 'Модели двигателей: '\n",
        "                ul = div.find('ul')\n",
        "                for t in stringstolist(ul):\n",
        "                    text += t + ', '\n",
        "                text = text[:-2] + '\\n\\n'\n",
        "\n",
        "            return text\n",
        "\n",
        "        def parseroptions(url):\n",
        "            text = '## Комплектации\\n'\n",
        "            links = []\n",
        "            url = url.replace('desc', 'options')\n",
        "            suop = getmodelsoup(url)\n",
        "            ahrefs = suop.findAll('div', class_='config__variants__slide')\n",
        "            for a in ahrefs:\n",
        "                cont = a.find('li').text\n",
        "                url = a.find('a')\n",
        "                option = getmodelsoup('https://www.kia.ru' + url['href'])\n",
        "                titel = option.find('title').text\n",
        "                text += f'### Комплектация: {titel}\\nЦена: {cont}\\n'\n",
        "                info = option.findAll('div', class_=\"info-section\")\n",
        "\n",
        "                for i in info:\n",
        "                    t2 = i.find('div', class_=\"info-section__header\").text\n",
        "                    text += t2 + ': '\n",
        "                    if t2.strip() == 'Технические характеристики' or t2.strip() == 'Спецификация':\n",
        "                        dl = i.findAll('dl')\n",
        "                        for j in dl:\n",
        "                            text += j.find('dt').text + ': ' + j.find('dd').text + '; '\n",
        "                    else:\n",
        "                        li = i.findAll('li')\n",
        "                        for j in li:\n",
        "                            text += j.text + ', '\n",
        "                    text = text[:-2] + '\\n'\n",
        "\n",
        "            return text\n",
        "\n",
        "        def resultsmodel(link):\n",
        "\n",
        "            print(link)\n",
        "            name = modeldict[link].find('title').text.split(' – ')[0]\n",
        "            name = name.replace('/', '-')\n",
        "            text = f'#  {name} - [link {link}]\\n'\n",
        "            text += parsermodel(modeldict[link])\n",
        "            text += parseroptions(link)\n",
        "            savetxt('models', name + '.txt', text)\n",
        "            print('Done ' + name)\n",
        "\n",
        "        threads = []\n",
        "        # Добавляю потоки с функцией сохранения в файл в список потоков\n",
        "        for link in modellinks:\n",
        "            threads.append(threading.Thread(target=resultsmodel, args=(link,)))\n",
        "\n",
        "        # Технологии\n",
        "\n",
        "        def parserabout(soup):\n",
        "            text = '## '\n",
        "            pages = soup.find('div', class_='articles-detail__technology-txt')\n",
        "            if pages is None:\n",
        "                print(soup.find('title').text.split(' – ')[0])\n",
        "                print(soup)\n",
        "            else:\n",
        "                content = pages.select('div[class*=\"text-\"]')\n",
        "                text += ''.join(i.text + '\\n' for i in content)\n",
        "            return text\n",
        "\n",
        "        url = \"https://www.kia.ru/ajax/page/technologies/more?limit=45&page=1\"\n",
        "\n",
        "        headers = {\n",
        "            \"Referer\": \"https://www.kia.ru/about/technologies/\",  # Example Referer header\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, headers=headers)\n",
        "        json_data = response.text\n",
        "\n",
        "        data = json.loads(json_data)\n",
        "\n",
        "        ids = [tech['id'] for tech in data['content']['technologies']]\n",
        "\n",
        "        static_url = 'https://www.kia.ru/about/technologies/'\n",
        "        urls = []\n",
        "\n",
        "        for id in ids:\n",
        "            urls.append(static_url + id)\n",
        "\n",
        "        def resultsabout(link):\n",
        "\n",
        "            print(link)\n",
        "            content = requests.get(link)\n",
        "            if content.status_code == 200:\n",
        "                # constructor-block\n",
        "                text = ''\n",
        "                html = ''.join(line.strip() for line in content.text.split(\"\\n\"))\n",
        "                soup = BeautifulSoup(html, \"html.parser\")\n",
        "                name = soup.find('title').text.split(' – ')[0]\n",
        "                text += parserabout(soup)\n",
        "            savetxt('about', name.replace('/', '-') + '.txt', text)\n",
        "            print('Done ' + name)\n",
        "\n",
        "        print(urls)\n",
        "        for link in urls:\n",
        "            # Добавляю потоки с функцией сохранения в файл в список потоков\n",
        "            threads.append(threading.Thread(target=resultsabout, args=(link,)))\n",
        "\n",
        "        # Сбор с закладки \"Журнал\"\n",
        "\n",
        "        url = \"https://www.kia.ru/ajax/page/mediacenter/magazine/more?limit=100&page=1\"\n",
        "        static_url = \"https://www.kia.ru/press/magazine/\"\n",
        "        HEADERS = {\"Referer\": static_url}\n",
        "\n",
        "        response = requests.get(url=url, headers=HEADERS)\n",
        "        json_data = response.text\n",
        "        data = json.loads(json_data)\n",
        "        all_article_list = []\n",
        "\n",
        "        for article in data[\"content\"][\"media_center\"][\"magazine\"]:\n",
        "            code = article[\"code\"]\n",
        "            all_article_list.append(static_url + code + '/')\n",
        "\n",
        "        url = \"https://www.kia.ru/ajax/page/mediacenter/news/more?limit=100&page=1\"\n",
        "        static_url = \"https://www.kia.ru/press/news/\"\n",
        "        response = requests.get(url=url, headers=HEADERS)\n",
        "        json_data = response.text\n",
        "        data = json.loads(json_data)\n",
        "        all_article_list = []\n",
        "\n",
        "        for article in data[\"content\"][\"media_center\"][\"news\"]:\n",
        "            code = article[\"code\"]\n",
        "            all_article_list.append(static_url + code + '/')\n",
        "\n",
        "        def resultspress(link):\n",
        "\n",
        "            print(link)\n",
        "            content = requests.get(link)\n",
        "            if content.status_code == 200:\n",
        "                # constructor-block\n",
        "                text = ''\n",
        "                html = ''.join(line.strip() for line in content.text.split(\"\\n\"))\n",
        "                soup = BeautifulSoup(html, \"html.parser\")\n",
        "                name = soup.find('title').text.split(' – ')[0]\n",
        "                head_press = soup.h1.text.strip()  # заголовок статьи\n",
        "                all_img_list, all_img = [], \"\"\n",
        "                try:\n",
        "                    [all_img_list.append(img.find(\"img\").attrs.get(\"src\")) for img in\n",
        "                     soup.find_all(\"div\", class_=\"articles-detail__content__offset\")]  # все фото из статьи\n",
        "                    for img in all_img_list:\n",
        "                        all_img += img + \",\"\n",
        "                except:\n",
        "                    all_img = \"\"\n",
        "\n",
        "                try:\n",
        "                    date_press = soup.select_one(\"div.articles-detail__date\").text.strip()  # дата статьи\n",
        "                    for val in soup.find_all(\"div\", class_=\"g-container\"):\n",
        "                        for child in val.children:\n",
        "                            if date_press in child.text:\n",
        "                                text += f\"## {head_press}>\\n\"\n",
        "                                text += child.text.replace(\"\\xa0\", \" \")\n",
        "                                text += f\"\\nФотографии из статьи: {all_img[:-1]}\\n\\n\"\n",
        "                except:\n",
        "                    for val in soup.find_all(\"div\", class_=\"g-container\"):\n",
        "                        for child in val.children:\n",
        "                            if child.find(\"h1\") is not None and child.find(\"h1\") != -1:\n",
        "                                text += f\"## {head_press}\\n\"\n",
        "                                text += child.text.replace(\"\\xa0\", \" \")\n",
        "                                text += f\"\\nФотографии из статьи: {all_img[:-1]}\\n\\n\"\n",
        "                if text != '':\n",
        "                    savetxt('press', name.replace('/', '-') + '.txt', text)\n",
        "                    print('Done ' + name)\n",
        "\n",
        "        # threads = []\n",
        "        for link in all_article_list:\n",
        "            threads.append(threading.Thread(target=resultspress, args=(link,)))\n",
        "\n",
        "        # Start all threads\n",
        "        for x in threads:\n",
        "            x.start()\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        # Wait for all of them to finish\n",
        "        for x in threads:\n",
        "            x.join()\n",
        "\n",
        "        # Тест ассортимента\n",
        "\n",
        "        base_url = \"https://www.kia.ru/ajax/page/accessories/filter?sort=sort&order=desc&page=1\"\n",
        "\n",
        "        headers = {\n",
        "            \"Referer\": \"https://www.kia.ru/service/accessories/\",  # Example Referer header\n",
        "        }\n",
        "\n",
        "        bigdata = []\n",
        "\n",
        "        start_page = 1\n",
        "\n",
        "        while True:\n",
        "            current_url = base_url + str(start_page)\n",
        "            response = requests.get(current_url, headers=headers)\n",
        "            json_data = response.text\n",
        "            data = json.loads(json_data)\n",
        "            if len(data['content']['accessories']) == 0: break\n",
        "\n",
        "            for tech in data['content']['accessories']:\n",
        "                bigdata.append(tech)\n",
        "\n",
        "            start_page += 1\n",
        "\n",
        "        text = '# Аксесуары\\n'\n",
        "        for i in bigdata:\n",
        "            if i['material'] == '':\n",
        "                i['material'] = {}\n",
        "                i['material']['name'] = ''\n",
        "            text += f'## Наименование: {i[\"name\"].strip()}, Фото: https://cdn.kia.ru/resize/1295x632{i[\"image\"]},'\n",
        "            text += f'Артикул: {i[\"article\"]}, Материал: {i[\"material\"][\"name\"].strip()}.\\n{i[\"text\"]}'\n",
        "            if i['technical_features'] is not None:\n",
        "                html = ''.join(line.strip() for line in i['technical_features'].split(\"\\n\"))\n",
        "                soup = BeautifulSoup(html, \"html.parser\")\n",
        "                technical_features = '. '.join(soup.strings)\n",
        "            text += technical_features + '\\n'\n",
        "        savetxt('accessories', 'accessories.txt', text)\n",
        "\n",
        "        # Test oil page\n",
        "        oil_res = {}\n",
        "        headers = {\"Referer\": 'https://www.kia.ru/'}\n",
        "\n",
        "        json_data = json.loads(requests.get('https://www.kia.ru/ajax/decoder/model_lines',\n",
        "                                            headers={\"Referer\": 'https://www.kia.ru/'}).text)\n",
        "        model_lines = list(json_data['content']['model_lines'])\n",
        "        print(datetime.datetime.now())\n",
        "        for id in model_lines:\n",
        "            try:\n",
        "                model_line_id = id['id']\n",
        "                json_data = json.loads(requests.get('https://www.kia.ru/ajax/decoder/years',\n",
        "                                                    params={'model_line_id': model_line_id},\n",
        "                                                    headers=headers).text)\n",
        "                years = list(json_data['content']['years'])\n",
        "                for year in years:\n",
        "                    year = year['id']\n",
        "                    json_data = json.loads(requests.get('https://www.kia.ru/ajax/decoder/models',\n",
        "                                                        params={'model_line_id': model_line_id, 'year': year},\n",
        "                                                        headers=headers).text)\n",
        "                    models = list(json_data['content']['models'])\n",
        "                    for model in models:\n",
        "                        model_id = model['id']\n",
        "                        json_data = json.loads(requests.get('https://www.kia.ru/ajax/decoder/complectations',\n",
        "                                                            params={'model_id': model_id},\n",
        "                                                            headers=headers).text)\n",
        "                        complectations = list(json_data['content']['complectations'])\n",
        "                        for complectation in complectations:\n",
        "                            complectation_id = complectation['id']\n",
        "                            json_data = json.loads(\n",
        "                                requests.get('https://www.kia.ru/ajax/page/oils/complectations/' + complectation_id,\n",
        "                                             headers=headers).text)\n",
        "                            oils = list(json_data['content']['oils'])\n",
        "                            car = json_data['content']['car']\n",
        "                            if oils != []:\n",
        "                                for oil in oils:\n",
        "                                    com_name = car['model']['model_line']['name'] + ' '\n",
        "                                    com_name += car['model']['generation']['name'] + '/ ' + str(year) + '/ '\n",
        "                                    com_name += car['model']['carcass']['name'] + '/ '\n",
        "                                    com_name += str(car['modification']['engine']['engine_volume']) + ' '\n",
        "                                    com_name += car['modification']['engine']['engine_type'] + '/ '\n",
        "                                    com_name += car['modification']['engine']['fuel_type'] + '/ '\n",
        "                                    com_name += car['modification']['transmission']['drive'] + '/ '\n",
        "                                    com_name += car['modification']['transmission']['gearbox'] + '/ '\n",
        "\n",
        "                                    if not oil['name'] in oil_res.keys():\n",
        "                                        oil_res[oil['name']] = [oil['description'], com_name]\n",
        "                                    else:\n",
        "                                        oil_res[oil['name']] += [com_name]\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "        print(oil_res)\n",
        "        print(datetime.datetime.now())\n",
        "\n",
        "\n",
        "        url = \"https://www.kia.ru/kiaflex/\"\n",
        "        response = requests.get(url=url, headers=HEADERS).text\n",
        "        html = ''.join(line.strip() for line in response.split(\"\\n\"))\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "        savetxt('kiaflex', 'kiaflex.txt', '# ' + parser(soup))\n",
        "\n",
        "        for oil in oil_res:\n",
        "            print(oil, oil_res[oil][0])\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"!echo \"# Технологии\" > database.txt\"\"\"\n",
        "        \"\"\"!cat result/about/*.txt >> database.txt\"\"\"\n",
        "        \"\"\"!cat result/press/*.txt >> database.txt\"\"\"\n",
        "        \"\"\"!cat result/models/*.txt >> database.txt\"\"\"\n",
        "        \"\"\"!cat result/kiaflex/*.txt >> database.txt\"\"\"\n",
        "        \"\"\"!cat result/accessories/*.txt >> database.txt\"\"\"\n",
        "        \"\"\"!zip -r result.zip result database.txt\"\"\"\n",
        "        \"\"\"!zip database.zip database.txt\"\"\"\n",
        "\n",
        "        print(msg, \" ... OK\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWMAEtbWEm2Z",
        "outputId": "11a2f283-8c0e-4c85-e986-d3aa88644717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ... OK\n"
          ]
        }
      ]
    }
  ]
}