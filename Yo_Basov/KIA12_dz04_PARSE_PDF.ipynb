{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ЧЕРНОВОЙ ВАРИАНТ парсит pdf веб сайта\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3uUDjfMwvJ_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"config.txt\", \"w\") as fw:\n",
        "    fw.write(\"[COLAB]\\n\")\n",
        "    fw.write(\"knowledge_dir=/content/drive/MyDrive/KIA_AVATAR/Scripts/knowledge\\n\")\n",
        "    fw.write(\"prev_knowledge_dir=/content/drive/MyDrive/KIA_AVATAR/Scripts/../knowledge\\n\")\n",
        "    fw.write(\"\\n\")\n",
        "    fw.write(\"[PROM]\\n\")\n",
        "    fw.write(\"knowledge_dir=/opt/knowledge\\n\")\n",
        "    fw.write(\"prev_knowledge_dir=/knowledge\\n\")\n",
        "    fw.write(\"\\n\")\n",
        "    fw.write(\"[STEP01_COPY_EXCEL]\\n\")\n",
        "    fw.write(\"api_key=?\\n\")\n",
        "    fw.write(\"neuro_copyrighter_limit=0\\n\")\n",
        "    fw.write(\"\\n\")\n",
        "    fw.write(\"[STEP03_COPY_WEBSITE]\\n\")\n",
        "    fw.write(\"api_key=?\\n\")\n",
        "    fw.write(\"neuro_copyrighter_limit=0\\n\")\n",
        "    fw.write(\"\\n\")\n",
        "    fw.write(\"[STEP05_COPY_PDF]\\n\")\n",
        "    fw.write(\"api_key=?\\n\")\n",
        "    fw.write(\"neuro_copyrighter_limit=0\\n\")\n",
        "\n",
        "msg = \"\""
      ],
      "metadata": {
        "id": "HrHdW92ivJ8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "        import configparser\n",
        "        import pathlib\n",
        "        import shutil\n",
        "        import os\n",
        "        config = configparser.ConfigParser()\n",
        "        config.read('config.txt')\n",
        "        knowledge_dir = config[\"COLAB\"][\"knowledge_dir\"]\n",
        "        if knowledge_dir is None: knowledge_dir = \"./knowledge\"\n",
        "        pathlib.Path(knowledge_dir).mkdir(parents=True, exist_ok=True)\n",
        "        prev_knowledge_dir = config[\"COLAB\"][\"prev_knowledge_dir\"]\n",
        "        if prev_knowledge_dir is None: prev_knowledge_dir = \"../knowledge\"\n",
        "        pathlib.Path(prev_knowledge_dir).mkdir(parents=True, exist_ok=True)\n",
        "        shutil.copyfile(os.path.join(prev_knowledge_dir, \"pdf_database.txt\"), os.path.join(knowledge_dir, \"pdf_database.txt\"))\n",
        "        \"\"\"\n",
        "        # импортируем нужные библиотеки\n",
        "        import requests\n",
        "        from bs4 import BeautifulSoup\n",
        "        import re\n",
        "        import numpy as np\n",
        "        import os\n",
        "        from tika import parser\n",
        "\n",
        "\n",
        "\n",
        "        # cкачиваем фалы данных в виде csv\n",
        "\n",
        "        def load_document_csv(url: str) -> str:\n",
        "            # Extract the document ID from the URL\n",
        "            match_ = re.search('/spreadsheets/d/([a-zA-Z0-9-_]+)', url)\n",
        "            if match_ is None:\n",
        "                raise ValueError('Invalid Google Docs URL')\n",
        "            doc_id = match_.group(1)\n",
        "\n",
        "            # Download the document as plain text\n",
        "            response = requests.get(f'https://docs.google.com/spreadsheets/d/{doc_id}/export?format=tsv')\n",
        "            response.raise_for_status()\n",
        "            text = response.content\n",
        "\n",
        "            return text\n",
        "\n",
        "        urls = load_document_csv('https://docs.google.com/spreadsheets/d/1h5BNBTXwzb8nMTIcre0vCDcujax1Fvfw3RdwT7FeAt8/edit?usp=sharing')\n",
        "        with open('urls_r.csv','wb') as f:\n",
        "          f.write(urls)\n",
        "\n",
        "        # загружаем ссылки из файла данных в массим Numpy\n",
        "        links = np.genfromtxt('urls_r.csv', delimiter='\\t', dtype=str, encoding='utf-8' )\n",
        "\n",
        "\n",
        "        '''\n",
        "        Эта функция находит все файлы pdf на странице сайта.\n",
        "\n",
        "        Вход:\n",
        "          url - адрес страницы сайта\n",
        "        Выход:\n",
        "          title - заголовок страницы\n",
        "          PDF_name - словарь в котором key - это название тега, value - ссылка на файл\n",
        "        '''\n",
        "\n",
        "        def find_pdf(url: str) -> str:\n",
        "\n",
        "          # заводим переменную для названия страницы сайта\n",
        "          title = \"\"\n",
        "          # Заводим пустой словарь для списка pdf файлов\n",
        "          PDF_name = {}\n",
        "\n",
        "          # Получаем запрос от страницы\n",
        "          response = requests.get(url)\n",
        "          # Получаем ПрекрасныйСуп\n",
        "          bs = BeautifulSoup(response.text, \"html.parser\")\n",
        "          # сохраняем заголовок страницы\n",
        "          title = bs.title.string\n",
        "\n",
        "          # Определяем функцию отбора тегов. Берём теги у которых есть ссылка.\n",
        "          def have_href(href):\n",
        "            return href\n",
        "\n",
        "          # Отбитаем теги у которых есть ссылка\n",
        "          bs.find_all(href = have_href)\n",
        "\n",
        "\n",
        "          # Наполняем словарь описаниями файла и ссылками\n",
        "          # То, что это словарь - важно.\n",
        "          # Так мы избегаем одинаковых файлов, но с разными ссылками. (такое есть)\n",
        "          for tag in bs.find_all(href = have_href):\n",
        "              # Извлекаем ссылку из тега\n",
        "              tag_href = tag.attrs['href']\n",
        "              # Делаем условие, если ссылка на файл, который оканчивается на .pdf\n",
        "              if tag_href.split('.')[-1] == 'pdf' and \\\n",
        "                (\"Обзор функций (PDF)\" not in tag.get_text()): # это условие только для сайта KIA\n",
        "                # Добавляем новый элемент в словарь\n",
        "                PDF_name[tag.get_text().strip()] = tag_href\n",
        "          # Возвращаем словарь\n",
        "          return title, PDF_name\n",
        "\n",
        "          # Смотрим, как работает функция на примере одной станицы сайта:\n",
        "\n",
        "          '''\n",
        "          Эта функция принимает на вход URL-адрес\n",
        "          по которому находится pdf файл, переводит его в текстовый формат\n",
        "          при помощи tika и сохраняет на диск.\n",
        "\n",
        "            Вход:\n",
        "              url - адрес PDF файла\n",
        "            Выход:\n",
        "              topic - текст\n",
        "          '''\n",
        "\n",
        "          def pdf_to_txt_tika(url):\n",
        "            topic = ''\n",
        "            # pattern = re.compile(r'\\w[а-я]')\n",
        "            # получаем файл\n",
        "            pdf_req = requests.get(url)\n",
        "            # проверяем, что ссылка удачно парсится\n",
        "            if pdf_req.status_code == 200:\n",
        "              # запоминаем название файла без расширения\n",
        "              f_name = os.path.basename(url).split('.')[0]\n",
        "              # записываем файл на диск\n",
        "              with open(f'{f_name}.pdf','wb') as f:\n",
        "                f.write(pdf_req.content)\n",
        "\n",
        "              # парсим pdf файл\n",
        "              reader = parser.from_file(f'{f_name}.pdf')\n",
        "              # сохраняем текст в переменную\n",
        "              text = reader['content']\n",
        "              # убираем спецсимволы\n",
        "              text = re.sub(u\"\\uFFFD\",' ', text)\n",
        "              # убираем повторяющиеся пробелы внутри строки\n",
        "              text = re.sub('\\u0020+',' ', text)\n",
        "              # убираем ошибочные разрывы абзацев\n",
        "              text = text.replace(' \\n\\n', '\\n')\n",
        "              text = text.replace('. \\n', '\\n')\n",
        "              text = text.replace(': \\n', ':\\n')\n",
        "              text = text.replace('; \\n', ';\\n')\n",
        "              text = text.replace(' \\n', ' ')\n",
        "              text = text.replace('-\\n', '-')\n",
        "              # разбиваем текст на строки для дальнейшей обработки\n",
        "              txt_split = text.split('\\n')\n",
        "\n",
        "              # начинаем исправлять типичные ошибки парсера\n",
        "              for st in txt_split:\n",
        "                # убираем пробелы в начале строки\n",
        "                st = st.lstrip()\n",
        "                # проверям, что строка это не номер страницы\n",
        "                num = re.match(\"^[0-9\\s*]+$\", st)\n",
        "                # если строка не пустая и не номер страницы - добавляем её к тексту\n",
        "                if not num and st:\n",
        "                  topic += st + '\\n'\n",
        "\n",
        "              topic = topic.replace(u' \\u2022', u'\\n\\u2022')\n",
        "\n",
        "              # снова разбиваем текст на строки для дальнейшей обработки\n",
        "              txt_split = topic.split('\\n')\n",
        "              topic = ''\n",
        "\n",
        "              for st in txt_split:\n",
        "                if st:\n",
        "                  if re.match('[а-яёa-zA-Z\\u00AB]', st[0]) and st:\n",
        "                    topic += ' ' + st\n",
        "                  else:\n",
        "                    topic += '\\n' + st\n",
        "\n",
        "\n",
        "              # и снова разбиваем текст, чтобы убрать повторы\n",
        "              txt_split = topic.split('\\n')\n",
        "              topic = ''\n",
        "              last_st = ''\n",
        "\n",
        "              for st in txt_split:\n",
        "                if last_st != st:\n",
        "                  topic +=  st + '\\n'\n",
        "                last_st = st\n",
        "\n",
        "\n",
        "              # стираем pdf файл\n",
        "              if os.path.isfile(f'{f_name}.pdf'):\n",
        "                os.remove(f'{f_name}.pdf')\n",
        "\n",
        "              # записываем текст в файл с названием оригинала, но txt\n",
        "              with open(f'{f_name}.txt','w') as f:\n",
        "                f.write(topic)\n",
        "\n",
        "              return topic\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        print(msg, \" ... OK\")\n"
      ],
      "metadata": {
        "id": "A0mqbbS6vJ6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pLw6urqRvJ3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bw9BRRHyvJ1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6_Z-QcQwvJy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tmd0kLc5vJwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_zmRXOhWvJtp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}