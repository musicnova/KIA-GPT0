{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ЧЕРНОВОЙ ВАРИАНТ добавления ссылок на сайт\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CZ7Cy75McqkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/drive/MyDrive/KIA_TEST/HOTFIX_MODELS_LINKS\n",
        "!mkdir -p /content/drive/MyDrive/KIA_TEST/HOTFIX_MODELS_LINKS\n",
        "!git clone -b hotfix_models_links https://github.com/musicnova/KIA-GPT0.git HOTFIX_MODELS_LINKS\n",
        "!cp -r /content/HOTFIX_MODELS_LINKS/knowledge/ /content/drive/MyDrive/KIA_TEST/HOTFIX_MODELS_LINKS/knowledge\n",
        "!rm -r /content/HOTFIX_MODELS_LINKS"
      ],
      "metadata": {
        "id": "3DJQUSsMMfZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken==0.4.0 langchain==0.0.231 openai==0.27.8 faiss-cpu==1.7.4"
      ],
      "metadata": {
        "id": "-KXFwB2BDYzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.text_splitter import NLTKTextSplitter\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "\n",
        "with open('/content/drive/MyDrive/KIA_TEST/HOTFIX_MODELS_LINKS/knowledge/database.md', 'r', encoding='utf-8') as file:\n",
        "    text = '\\n'.join([\"\" if line.startswith(\"~~~~~ \") else line for line in file.readlines()]).replace('\\r', '')\n",
        "\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "    (\"###\", \"Header 3\"),\n",
        "    (\"####\", \"Header 4\"),\n",
        "    (\"#####\", \"Header 5\"), # <--- splitter for 4096 tokens chunks\n",
        "]\n",
        "\n",
        "def meta_to_str(m):\n",
        "    return '\\t'.join([str(m[\"Header 1\"]) if \"Header 1\" in m else \"None\"\n",
        "        ,str(m[\"Header 2\"]) if \"Header 2\" in m else \"None\"\n",
        "        ,str(m[\"Header 3\"]) if \"Header 3\" in m else \"None\"\n",
        "        ,str(m[\"Header 4\"]) if \"Header 4\" in m else \"None\"])\n",
        "\n",
        "def str_to_meta(s):\n",
        "    a = s.split('\\t')\n",
        "    return {\"Header 1\": a[0] if a[0] != 'None' else None\n",
        "            , \"Header 2\": a[1] if a[1] != 'None' else None\n",
        "            , \"Header 3\": a[2] if a[2] != 'None' else None\n",
        "            , \"Header 4\": a[3] if a[3] != 'None' else None}\n",
        "\n",
        "# Markdown-level splits\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "md_header_splits = markdown_splitter.split_text(text)\n",
        "md_header_splits_dict = {}\n",
        "for md_header_split in md_header_splits:\n",
        "    md_header_splits_key = meta_to_str(md_header_split.metadata)\n",
        "    if md_header_splits_key in md_header_splits_dict:\n",
        "        md_header_splits_dict[md_header_splits_key].append(md_header_split.page_content)\n",
        "    else:\n",
        "        md_header_splits_dict[md_header_splits_key] = [md_header_split.page_content]\n",
        "\n",
        "# Character-level splits\n",
        "char_splitter = CharacterTextSplitter(separator='\\r\\n', chunk_size=4096, chunk_overlap=0)\n",
        "char_splits_dict = {}\n",
        "for md_header_splits_key in md_header_splits_dict:\n",
        "    for md_header_splits_value in md_header_splits_dict[md_header_splits_key]:\n",
        "        char_splits = char_splitter.split_text(md_header_splits_value)\n",
        "        for char_split in char_splits:\n",
        "            if md_header_splits_key in char_splits_dict:\n",
        "                char_splits_dict[md_header_splits_key].append(char_split)\n",
        "            else:\n",
        "                char_splits_dict[md_header_splits_key] = [char_split]\n",
        "\n",
        "# Recursive Character-level splits\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4096, chunk_overlap=0)\n",
        "text_splits_dict = {}\n",
        "for char_splits_key in char_splits_dict:\n",
        "    for char_splits_value in char_splits_dict[char_splits_key]:\n",
        "        text_splits = text_splitter.split_text(char_splits_value)\n",
        "        for text_split in text_splits:\n",
        "            if char_splits_key in text_splits_dict:\n",
        "                text_splits_dict[char_splits_key].append(text_split)\n",
        "            else:\n",
        "                text_splits_dict[char_splits_key] = [text_split]\n",
        "\n",
        "# Splits\n",
        "splits = []\n",
        "for text_splits_key in text_splits_dict:\n",
        "    for text_splits_value in text_splits_dict[text_splits_key]:\n",
        "        splits.append(Document(page_content=text_splits_value, metadata=str_to_meta(text_splits_key)))\n",
        "print(len(splits))"
      ],
      "metadata": {
        "id": "kab_6GDyJ5NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_splits = []\n",
        "\n",
        "def get_words(text, random_seed):\n",
        "    import random\n",
        "    import re\n",
        "    random.seed(random_seed)\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    start = random.randint(0, len(words) - 12)\n",
        "    selected_words = [words[start + i] for i in range(0, 12)] if len(words) > 12 else []\n",
        "    return selected_words\n",
        "\n",
        "def duckduckgo(words):\n",
        "    import random\n",
        "    import time\n",
        "    import requests\n",
        "    from bs4 import BeautifulSoup\n",
        "    query = 'site:kia.ru ' + words\n",
        "    url = f\"https://duckduckgo.com/html/?q={query}\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    results = soup.select('.result__a')\n",
        "    time.sleep(random.randint(0, 2))\n",
        "    if results:\n",
        "        first_result = results[0]['href']\n",
        "        return first_result\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def googlego(words, your_engine_id, your_api_key):\n",
        "    # README https://help.elfsight.com/article/331-how-to-get-search-engine-id\n",
        "    # README https://stackoverflow.com/a/11100863\n",
        "    # README https://developers.google.com/custom-search/v1/introduction\n",
        "    import random\n",
        "    import time\n",
        "    from googleapiclient.discovery import build\n",
        "    query = 'site:kia.ru ' + words\n",
        "    cse_id = your_engine_id  # Замените на ваш идентификатор движка поиска\n",
        "    api_key = your_api_key  # Замените на ваш API-ключ\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
        "    res = service.cse().list(q=query, cx=cse_id, num=1).execute()\n",
        "    time.sleep(random.randint(0, 2))\n",
        "    if 'items' in res:\n",
        "        return res['items'][0]['link']\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "for split in splits:\n",
        "    if split.metadata['Header 1'].strip() != \"models\":\n",
        "        new_splits.append(split)\n",
        "    else:\n",
        "        print(str(split.metadata), '\\n')\n",
        "        print(split.page_content, '\\n')\n",
        "        counts = {}\n",
        "        max_count = 0\n",
        "        best_url = None\n",
        "        for seed in range(0, 5):\n",
        "            try:\n",
        "                words = ' '.join(get_words(split.page_content, seed))\n",
        "            except:\n",
        "                words = []\n",
        "\n",
        "            try:\n",
        "                if split.page_content.find('https://www.kia.ru') >= 0 or split.page_content.find('.pdf') >= 0:\n",
        "                    raise Exception(\"SKIP\")\n",
        "                url = googlego(words, 'ID1', 'API_KEY1')\n",
        "            except:\n",
        "                print(\"WARN1 LIMIT EXCEEDED\")\n",
        "                url = None\n",
        "                try:\n",
        "                    if split.page_content.find('https://www.kia.ru') >= 0 or split.page_content.find('.pdf') >= 0:\n",
        "                        raise Exception(\"SKIP\")\n",
        "                    url = googlego(words, 'ID2', 'API_KEY2')\n",
        "                except:\n",
        "                    print(\"WARN2 LIMIT EXCEEDED\")\n",
        "                    url = None\n",
        "                    try:\n",
        "                        if split.page_content.find('https://www.kia.ru') >= 0 or split.page_content.find('.pdf') >= 0:\n",
        "                            raise Exception(\"SKIP\")\n",
        "                        url = googlego(words, 'ID3', 'API_KEY3')\n",
        "                    except:\n",
        "                        print(\"WARN3 LIMIT EXCEEDED\")\n",
        "                        url = None\n",
        "\n",
        "           # url = duckduckgo(words) # <- FIXME, delete this line, use google only\n",
        "           if url is not None:\n",
        "               print(words, ' -> ', url)\n",
        "               if url in counts:\n",
        "                   counts[url] += 1\n",
        "               else:\n",
        "                   counts[url] = 1\n",
        "               if counts[url] > max_count and url.find('news') < 0 and (url.find('https://www.kia.ru') >= 0 or url.find('.pdf') >= 0):\n",
        "                   max_count = counts[url]\n",
        "                   best_url = url\n",
        "        print(\"=====\", '\\n')\n",
        "        if best_url is not None and split.page_content.find(best_url) < 0:\n",
        "            new_page_content=split.page_content + \"\\n\" + str(best_url) + \"\\n\"\n",
        "            print(best_url, '\\n')\n",
        "            new_splits.append(Document(page_content=new_page_content, metadata=split.metadata))\n",
        "        else:\n",
        "            new_splits.append(Document(page_content=split.page_content, metadata=split.metadata))\n",
        "        print(\"=====\", '\\n')"
      ],
      "metadata": {
        "id": "WpdxEKWPL3Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import binascii\n",
        "\n",
        "new_text_splits_dict = {}\n",
        "for new_split in new_splits:\n",
        "    new_text_splits_key = meta_to_str(new_split.metadata)\n",
        "    if new_text_splits_key in new_text_splits_dict:\n",
        "        new_text_splits_dict[new_text_splits_key].append(new_split.page_content)\n",
        "    else:\n",
        "        new_text_splits_dict[new_text_splits_key] = [new_split.page_content]\n",
        "\n",
        "for cur in range(0, len(new_splits)):\n",
        "    new_split = new_splits[cur]\n",
        "    new_text_splits_key = meta_to_str(new_split.metadata)\n",
        "    new_text_splits_index = new_text_splits_dict[new_text_splits_key].index(new_split.page_content)\n",
        "    if len(new_text_splits_dict[new_text_splits_key]) > 1:\n",
        "        new_split.metadata['Header 5'] = ','.join([str(binascii.crc32(new_splits[cur + new_text_splits_index - i].page_content.encode('utf8'))) if i != new_text_splits_index else \"...\" for i in range(0, len(new_text_splits_dict[new_text_splits_key]))])\n",
        "\n",
        "with open(\"/content/drive/MyDrive/KIA_TEST/HOTFIX_MODELS_LINKS/knowledge/new_database.md\", \"w\") as f:\n",
        "    header_1 = None\n",
        "    header_2 = None\n",
        "    header_3 = None\n",
        "    header_4 = None\n",
        "    header_5 = None\n",
        "    for new_split in new_splits:\n",
        "        new_header_1 = new_split.metadata['Header 1']\n",
        "        new_header_2 = new_split.metadata['Header 2']\n",
        "        new_header_3 = new_split.metadata['Header 3']\n",
        "        new_header_4 = new_split.metadata['Header 4']\n",
        "        new_header_5 = new_split.metadata['Header 5']\n",
        "        if new_header_1 is not None and header_1 != new_header_1:\n",
        "            f.write('# ' + new_header_1 + '\\n')\n",
        "            header_1 = new_header_1\n",
        "        if new_header_2 is not None and header_2 != new_header_2:\n",
        "            f.write('## ' + new_header_2 + '\\n')\n",
        "            header_2 = new_header_2\n",
        "        if new_header_3 is not None and header_3 != new_header_3:\n",
        "            f.write('### ' + new_header_3 + '\\n')\n",
        "            header_3 = new_header_3\n",
        "        if new_header_4 is not None and header_4 != new_header_4:\n",
        "            f.write('#### ' + new_header_4 + '\\n')\n",
        "            header_4 = new_header_4\n",
        "        if new_header_5 is not None and header_5 != new_header_5:\n",
        "            f.write('##### ' + new_header_5 + '\\n')\n",
        "            header_5 = new_header_5\n",
        "        f.write(new_split.page_content + '\\n')\n",
        "\n",
        "    print(\"OK\")"
      ],
      "metadata": {
        "id": "BpD6IB0b5i0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wwJGDIz90l6m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}