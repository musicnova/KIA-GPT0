{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ЧЕРНОВОЙ ВАРИАНТ добавления ссылок на сайт\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ7Cy75McqkB",
        "outputId": "88d6654e-3883-4998-979c-d9d9afd90724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/drive/MyDrive/KIA_TEST/HOTFIX_MODELS_LINKS\n",
        "!mkdir -p /content/drive/MyDrive/KIA_TEST/HOTFIX_MODELS_LINKS\n",
        "!git clone -b hotfix_models_links https://github.com/musicnova/KIA-GPT0.git HOTFIX_MODELS_LINKS\n",
        "!cp -r /content/HOTFIX_MODELS_LINKS/knowledge/ /content/drive/MyDrive/KIA_TEST/HOTFIX_MODELS_LINKS/knowledge\n",
        "!rm -r /content/HOTFIX_MODELS_LINKS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DJQUSsMMfZ_",
        "outputId": "a7a5336f-0cf5-428d-df17-5afe62f9b748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HOTFIX_MODELS_LINKS'...\n",
            "remote: Enumerating objects: 2560, done.\u001b[K\n",
            "remote: Counting objects: 100% (973/973), done.\u001b[K\n",
            "remote: Compressing objects: 100% (425/425), done.\u001b[K\n",
            "remote: Total 2560 (delta 628), reused 820 (delta 541), pack-reused 1587\u001b[K\n",
            "Receiving objects: 100% (2560/2560), 64.97 MiB | 29.05 MiB/s, done.\n",
            "Resolving deltas: 100% (1472/1472), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken==0.4.0 langchain==0.0.231 openai==0.27.8 faiss-cpu==1.7.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KXFwB2BDYzd",
        "outputId": "710159be-b908-4654-c695-e08dad0b6d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken==0.4.0\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain==0.0.231\n",
            "  Downloading langchain-0.0.231-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai==0.27.8\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu==1.7.4\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.4.0) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.4.0) (2.31.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (3.8.6)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.231)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk<0.0.21,>=0.0.20 (from langchain==0.0.231)\n",
            "  Downloading langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (2.8.7)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (1.23.5)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.231)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (1.10.13)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.231) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (4.66.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.231) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.231)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.231)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.231) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.231) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.231) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.231)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: faiss-cpu, mypy-extensions, marshmallow, typing-inspect, tiktoken, openapi-schema-pydantic, langchainplus-sdk, openai, dataclasses-json, langchain\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.5.14 faiss-cpu-1.7.4 langchain-0.0.231 langchainplus-sdk-0.0.20 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-0.27.8 openapi-schema-pydantic-1.2.4 tiktoken-0.4.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.text_splitter import NLTKTextSplitter\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "\n",
        "with open('/content/drive/MyDrive/KIA_TEST/HOTFIX_MODELS_LINKS/knowledge/database.md', 'r', encoding='utf-8') as file:\n",
        "    text = file.read().replace('\\r', '')\n",
        "\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "    (\"###\", \"Header 3\"),\n",
        "    (\"####\", \"Header 4\"),\n",
        "    (\"#####\", \"Header 5\"),\n",
        "]\n",
        "\n",
        "def meta_to_str(m):\n",
        "    return '\\t'.join([str(m[\"Header 1\"]) if \"Header 1\" in m else \"None\"\n",
        "        ,str(m[\"Header 2\"]) if \"Header 2\" in m else \"None\"\n",
        "        ,str(m[\"Header 3\"]) if \"Header 3\" in m else \"None\"\n",
        "        ,str(m[\"Header 4\"]) if \"Header 4\" in m else \"None\"\n",
        "        ,str(m[\"Header 5\"]) if \"Header 5\" in m else \"None\"])\n",
        "\n",
        "def str_to_meta(s):\n",
        "    a = s.split('\\t')\n",
        "    return {\"Header 1\": a[0] if a[0] != 'None' else None\n",
        "            , \"Header 2\": a[1] if a[1] != 'None' else None\n",
        "            , \"Header 3\": a[2] if a[2] != 'None' else None\n",
        "            , \"Header 4\": a[3] if a[3] != 'None' else None\n",
        "            , \"Header 5\": a[4] if a[4] != 'None' else None}\n",
        "\n",
        "# Markdown-level splits\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "md_header_splits = markdown_splitter.split_text(text)\n",
        "md_header_splits_dict = {}\n",
        "for md_header_split in md_header_splits:\n",
        "    md_header_splits_key = meta_to_str(md_header_split.metadata)\n",
        "    if md_header_splits_key in md_header_splits_dict:\n",
        "        md_header_splits_dict[md_header_splits_key].append(md_header_split.page_content)\n",
        "    else:\n",
        "        md_header_splits_dict[md_header_splits_key] = [md_header_split.page_content]\n",
        "\n",
        "# Character-level splits\n",
        "char_splitter = CharacterTextSplitter(separator='\\r\\n', chunk_size=4096, chunk_overlap=0)\n",
        "char_splits_dict = {}\n",
        "for md_header_splits_key in md_header_splits_dict:\n",
        "    for md_header_splits_value in md_header_splits_dict[md_header_splits_key]:\n",
        "        char_splits = char_splitter.split_text(md_header_splits_value)\n",
        "        for char_split in char_splits:\n",
        "            if md_header_splits_key in char_splits_dict:\n",
        "                char_splits_dict[md_header_splits_key].append(char_split)\n",
        "            else:\n",
        "                char_splits_dict[md_header_splits_key] = [char_split]\n",
        "\n",
        "# Recursive Character-level splits\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4096, chunk_overlap=0)\n",
        "text_splits_dict = {}\n",
        "for char_splits_key in char_splits_dict:\n",
        "    for char_splits_value in char_splits_dict[char_splits_key]:\n",
        "        text_splits = text_splitter.split_text(char_splits_value)\n",
        "        for text_split in text_splits:\n",
        "            if char_splits_key in text_splits_dict:\n",
        "                text_splits_dict[char_splits_key].append(text_split)\n",
        "            else:\n",
        "                text_splits_dict[char_splits_key] = [text_split]\n",
        "\n",
        "# Splits\n",
        "splits = []\n",
        "for text_splits_key in text_splits_dict:\n",
        "    for text_splits_value in text_splits_dict[text_splits_key]:\n",
        "        splits.append(Document(page_content=text_splits_value, metadata=str_to_meta(text_splits_key)))\n",
        "print(len(splits))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kab_6GDyJ5NW",
        "outputId": "26ef7aa3-7722-4003-8947-e4002f26c4d5"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_splits = []\n",
        "\n",
        "def get_words(text, random_seed):\n",
        "    import random\n",
        "    import re\n",
        "    random.seed(random_seed)\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    start = random.randint(0, len(words) - 6)\n",
        "    selected_words = [words[start + i] for i in range(0, 6)]\n",
        "    return selected_words\n",
        "\n",
        "def duckduckgo(words):\n",
        "    import random\n",
        "    import time\n",
        "    import requests\n",
        "    from bs4 import BeautifulSoup\n",
        "    time.sleep(random.randint(0, 2))\n",
        "    query = 'site:kia.ru ' + words\n",
        "    url = f\"https://duckduckgo.com/html/?q={query}\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    results = soup.select('.result__a')\n",
        "    if results:\n",
        "        first_result = results[0]['href']\n",
        "        return first_result\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def googlego(words, your_engine_id, your_api_key):\n",
        "    # README https://help.elfsight.com/article/331-how-to-get-search-engine-id\n",
        "    # README https://stackoverflow.com/a/11100863\n",
        "    # README https://developers.google.com/custom-search/v1/introduction\n",
        "    import random\n",
        "    import time\n",
        "    from googleapiclient.discovery import build\n",
        "    time.sleep(random.randint(0, 2))\n",
        "    query = 'site:kia.ru ' + words\n",
        "    cse_id = your_engine_id  # Замените на ваш идентификатор движка поиска\n",
        "    api_key = your_api_key  # Замените на ваш API-ключ\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
        "    res = service.cse().list(q=query, cx=cse_id, num=1).execute()\n",
        "    if 'items' in res:\n",
        "        return res['items'][0]['link']\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "for split in splits:\n",
        "    if split.metadata['Header 1'].strip() != \"models???\":\n",
        "        new_splits.append(split)\n",
        "    else:\n",
        "        print(str(split.metadata), '\\n')\n",
        "        print(split.page_content, '\\n')\n",
        "        counts = {}\n",
        "        max_count = 0\n",
        "        best_url = None\n",
        "        for seed in range(0, 5):\n",
        "           words = ' '.join(get_words(split.page_content, seed))\n",
        "           # url = googlego(words, 'ID', 'API_KEY')\n",
        "           url = duckduckgo(words)\n",
        "           if url is not None:\n",
        "               print(words, ' -> ', url)\n",
        "               if url in counts:\n",
        "                   counts[url] += 1\n",
        "               else:\n",
        "                   counts[url] = 1\n",
        "               if counts[url] > max_count:\n",
        "                   max_count = counts[url]\n",
        "                   best_url = url\n",
        "        print(\"=====\", '\\n')\n",
        "        if best_url is not None and split.page_content.find(best_url) < 0:\n",
        "            new_page_content=split.page_content + \"\\n\" + str(best_url) + \"\\n\"\n",
        "            print(best_url, '\\n')\n",
        "            new_splits.append(Document(page_content=new_page_content, metadata=split.metadata))\n",
        "        else:\n",
        "            new_splits.append(Document(page_content=split.page_content, metadata=split.metadata))\n",
        "        print(\"=====\", '\\n')"
      ],
      "metadata": {
        "id": "WpdxEKWPL3Gk"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import binascii\n",
        "\n",
        "new_text_splits_dict = {}\n",
        "for new_split in new_splits:\n",
        "    new_text_splits_key = meta_to_str(new_split.metadata)\n",
        "    if new_text_splits_key in new_text_splits_dict:\n",
        "        new_text_splits_dict[new_text_splits_key].append(new_split.page_content)\n",
        "    else:\n",
        "        new_text_splits_dict[new_text_splits_key] = [new_split.page_content]\n",
        "\n",
        "for cur in range(0, len(new_splits)):\n",
        "    new_split = new_splits[cur]\n",
        "    new_text_splits_key = meta_to_str(new_split.metadata)\n",
        "    new_text_splits_index = new_text_splits_dict[new_text_splits_key].index(new_split.page_content)\n",
        "    if len(new_text_splits_dict[new_text_splits_key]) > 1:\n",
        "        new_split.metadata['Header 5'] = ','.join([str(binascii.crc32(new_splits[cur + new_text_splits_index - i].page_content.encode('utf8'))) if i != new_text_splits_index else \"...\" for i in range(0, len(new_text_splits_dict[new_text_splits_key]))])\n",
        "\n",
        "with open(\"/content/drive/MyDrive/KIA_TEST/HOTFIX_MODELS_LINKS/knowledge/new_database.md\", \"w\") as f:\n",
        "    header_1 = None\n",
        "    header_2 = None\n",
        "    header_3 = None\n",
        "    header_4 = None\n",
        "    header_5 = None\n",
        "    for new_split in new_splits:\n",
        "        new_header_1 = new_split.metadata['Header 1']\n",
        "        new_header_2 = new_split.metadata['Header 2']\n",
        "        new_header_3 = new_split.metadata['Header 3']\n",
        "        new_header_4 = new_split.metadata['Header 4']\n",
        "        new_header_5 = new_split.metadata['Header 5']\n",
        "        if new_header_1 is not None and header_1 != new_header_1:\n",
        "            f.write('# ' + new_header_1 + '\\n')\n",
        "            header_1 = new_header_1\n",
        "        if new_header_2 is not None and header_2 != new_header_2:\n",
        "            f.write('## ' + new_header_2 + '\\n')\n",
        "            header_2 = new_header_2\n",
        "        if new_header_3 is not None and header_3 != new_header_3:\n",
        "            f.write('### ' + new_header_3 + '\\n')\n",
        "            header_3 = new_header_3\n",
        "        if new_header_4 is not None and header_4 != new_header_4:\n",
        "            f.write('#### ' + new_header_4 + '\\n')\n",
        "            header_4 = new_header_4\n",
        "        if new_header_5 is not None and header_5 != new_header_5:\n",
        "            f.write('##### ' + new_header_5 + '\\n')\n",
        "            header_5 = new_header_5\n",
        "        f.write(new_split.page_content + '\\n')\n",
        "\n",
        "    print(\"OK\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpD6IB0b5i0f",
        "outputId": "73c3e86b-901b-416e-8f64-a24cabe2b9c9"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wwJGDIz90l6m"
      },
      "execution_count": 126,
      "outputs": []
    }
  ]
}